{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from googletrans import Translator\n",
    "from collections import defaultdict\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize a list to store the data for each individual page\n",
    "all_data = {}\n",
    "\n",
    "# Initialize the web driver\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "# Initial search page URL\n",
    "url = \"https://samlingar.shm.se/sok?type=object&productionPeriod=Vikingatid&hasImage=1&category=Arkeologisk%20samling&category=Vapen%20och%20rustningar&listType=archaeological&rows=500&offset=0\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load (you may need to adjust the sleep duration)\n",
    "time.sleep(2)\n",
    "\n",
    "# Find the cookie disclaimer button by its aria-label\n",
    "cookie_disclaimer = driver.find_element(By.CSS_SELECTOR, '[aria-label=\"Godkänn alla kakor\"]')\n",
    "# Check if the cookie disclaimer button is displayed and then click it\n",
    "if cookie_disclaimer.is_displayed():\n",
    "    ActionChains(driver).move_to_element(cookie_disclaimer).click().perform()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data from the main table\n",
    "table = driver.find_element(By.TAG_NAME, \"table\")\n",
    "df = pd.read_html(table.get_attribute('outerHTML'))[0]\n",
    "\n",
    "# Drop the 'Bild' column\n",
    "if 'Bild' in df.columns:\n",
    "    df.drop(columns=['Bild'], inplace=True)\n",
    "\n",
    "# Extract museum names from the 'title' attribute of each <i> element\n",
    "museum_elements = driver.find_elements(By.CSS_SELECTOR, \"td i.museum-icon\")\n",
    "museum_names = [elem.get_attribute('title') for elem in museum_elements]\n",
    "\n",
    "# Replace the 'Museum' column with extracted text\n",
    "if 'Museum' in df.columns and len(museum_names) == len(df):\n",
    "    df['Museum'] = museum_names\n",
    "else:\n",
    "    print(\"Mismatch in number of rows while extracting museum names\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect names and href links for each item\n",
    "item_link = [(item.get_attribute('href')) for item in driver.find_elements(By.CLASS_NAME, \"archaeological-list__link\")]\n",
    "\n",
    "item_names = [f\"{item.text} - {index+1}\" for index, item in enumerate(driver.find_elements(By.CLASS_NAME, \"archaeological-list__link\"))]\n",
    "df['Unique Name'] = item_names\n",
    "df['Catalog Link'] = item_link\n",
    "\n",
    "# Initialize a column for extra details\n",
    "df['Extra Details'] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to each item's link and scrape additional details\n",
    "for index, link in enumerate(df['Catalog Link']):\n",
    "    driver.get(link)\n",
    "\n",
    "    # Scrape the details from the item's page\n",
    "    item_details = {}\n",
    "    item_tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
    "    for item_table in item_tables:\n",
    "        item_df = pd.read_html(item_table.get_attribute('outerHTML'))[0]\n",
    "        for row in item_df.itertuples(index=False):\n",
    "            item_details[row[0]] = row[1]\n",
    "\n",
    "    # Store the scraped details as JSON in the DataFrame\n",
    "    df.at[index, 'Extra Details'] = json.dumps(item_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Föremålsbenämning</th>\n",
       "      <th>Föremålsnr.</th>\n",
       "      <th>Förvärvsnr.</th>\n",
       "      <th>Andra nummer</th>\n",
       "      <th>Material</th>\n",
       "      <th>Plats</th>\n",
       "      <th>Fornlämning</th>\n",
       "      <th>Socken</th>\n",
       "      <th>Landskap</th>\n",
       "      <th>Land</th>\n",
       "      <th>Kontexttyp</th>\n",
       "      <th>Kontextnr.</th>\n",
       "      <th>Artbedömning</th>\n",
       "      <th>Benslagsbedömning</th>\n",
       "      <th>Museum</th>\n",
       "      <th>Unique Name</th>\n",
       "      <th>Catalog Link</th>\n",
       "      <th>Extra Details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spjut</td>\n",
       "      <td>371667_HST</td>\n",
       "      <td>34000</td>\n",
       "      <td>Undernummer: Bj 581</td>\n",
       "      <td>Järn</td>\n",
       "      <td>Björkö, Norr om Borg</td>\n",
       "      <td>L2017:1478</td>\n",
       "      <td>Adelsö socken</td>\n",
       "      <td>Uppland</td>\n",
       "      <td>Sverige</td>\n",
       "      <td>Kammargrav</td>\n",
       "      <td>581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Historiska museet</td>\n",
       "      <td>Spjut - 1</td>\n",
       "      <td>https://samlingar.shm.se/object/58928958-A16E-...</td>\n",
       "      <td>{\"Museum\": \"Historiska museet\", \"F\\u00f6rem\\u0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spjut</td>\n",
       "      <td>371668_HST</td>\n",
       "      <td>34000</td>\n",
       "      <td>Undernummer: Bj 581</td>\n",
       "      <td>Järn</td>\n",
       "      <td>Björkö, Norr om Borg</td>\n",
       "      <td>L2017:1478</td>\n",
       "      <td>Adelsö socken</td>\n",
       "      <td>Uppland</td>\n",
       "      <td>Sverige</td>\n",
       "      <td>Kammargrav</td>\n",
       "      <td>581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Historiska museet</td>\n",
       "      <td>Spjut - 2</td>\n",
       "      <td>https://samlingar.shm.se/object/EFCC758B-B6BB-...</td>\n",
       "      <td>{\"Museum\": \"Historiska museet\", \"F\\u00f6rem\\u0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Svärd Petersen Y</td>\n",
       "      <td>263086_HST</td>\n",
       "      <td>34000</td>\n",
       "      <td>Undernummer: Bj 752B</td>\n",
       "      <td>Järn</td>\n",
       "      <td>Björkö, Hemlanden</td>\n",
       "      <td>L2017:1904</td>\n",
       "      <td>Adelsö socken</td>\n",
       "      <td>Uppland</td>\n",
       "      <td>Sverige</td>\n",
       "      <td>Kammargrav</td>\n",
       "      <td>752B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Historiska museet</td>\n",
       "      <td>Svärd Petersen Y - 3</td>\n",
       "      <td>https://samlingar.shm.se/object/FFCD05E4-1460-...</td>\n",
       "      <td>{\"Museum\": \"Historiska museet\", \"F\\u00f6rem\\u0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tveeggat svärd</td>\n",
       "      <td>263468_HST</td>\n",
       "      <td>34000</td>\n",
       "      <td>Undernummer: Bj 542</td>\n",
       "      <td>Järn</td>\n",
       "      <td>Björkö, Norr om Borg</td>\n",
       "      <td>L2017:1478</td>\n",
       "      <td>Adelsö socken</td>\n",
       "      <td>Uppland</td>\n",
       "      <td>Sverige</td>\n",
       "      <td>Kammargrav</td>\n",
       "      <td>542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Historiska museet</td>\n",
       "      <td>Tveeggat svärd - 4</td>\n",
       "      <td>https://samlingar.shm.se/object/7D599E53-A81D-...</td>\n",
       "      <td>{\"Museum\": \"Historiska museet\", \"F\\u00f6rem\\u0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Svärd Petersen Y</td>\n",
       "      <td>264449_HST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FID: 264449</td>\n",
       "      <td>Järn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sverige</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Historiska museet</td>\n",
       "      <td>Svärd Petersen Y - 5</td>\n",
       "      <td>https://samlingar.shm.se/object/CD8214A7-2FFB-...</td>\n",
       "      <td>{\"Museum\": \"Historiska museet\", \"F\\u00f6rem\\u0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Föremålsbenämning Föremålsnr. Förvärvsnr.          Andra nummer Material  \\\n",
       "0             Spjut  371667_HST       34000   Undernummer: Bj 581     Järn   \n",
       "1             Spjut  371668_HST       34000   Undernummer: Bj 581     Järn   \n",
       "2  Svärd Petersen Y  263086_HST       34000  Undernummer: Bj 752B     Järn   \n",
       "3    Tveeggat svärd  263468_HST       34000   Undernummer: Bj 542     Järn   \n",
       "4  Svärd Petersen Y  264449_HST         NaN           FID: 264449     Järn   \n",
       "\n",
       "                  Plats Fornlämning         Socken Landskap     Land  \\\n",
       "0  Björkö, Norr om Borg  L2017:1478  Adelsö socken  Uppland  Sverige   \n",
       "1  Björkö, Norr om Borg  L2017:1478  Adelsö socken  Uppland  Sverige   \n",
       "2     Björkö, Hemlanden  L2017:1904  Adelsö socken  Uppland  Sverige   \n",
       "3  Björkö, Norr om Borg  L2017:1478  Adelsö socken  Uppland  Sverige   \n",
       "4                   NaN         NaN            NaN      NaN  Sverige   \n",
       "\n",
       "   Kontexttyp Kontextnr.  Artbedömning  Benslagsbedömning             Museum  \\\n",
       "0  Kammargrav        581           NaN                NaN  Historiska museet   \n",
       "1  Kammargrav        581           NaN                NaN  Historiska museet   \n",
       "2  Kammargrav       752B           NaN                NaN  Historiska museet   \n",
       "3  Kammargrav        542           NaN                NaN  Historiska museet   \n",
       "4         NaN        NaN           NaN                NaN  Historiska museet   \n",
       "\n",
       "            Unique Name                                       Catalog Link  \\\n",
       "0             Spjut - 1  https://samlingar.shm.se/object/58928958-A16E-...   \n",
       "1             Spjut - 2  https://samlingar.shm.se/object/EFCC758B-B6BB-...   \n",
       "2  Svärd Petersen Y - 3  https://samlingar.shm.se/object/FFCD05E4-1460-...   \n",
       "3    Tveeggat svärd - 4  https://samlingar.shm.se/object/7D599E53-A81D-...   \n",
       "4  Svärd Petersen Y - 5  https://samlingar.shm.se/object/CD8214A7-2FFB-...   \n",
       "\n",
       "                                       Extra Details  \n",
       "0  {\"Museum\": \"Historiska museet\", \"F\\u00f6rem\\u0...  \n",
       "1  {\"Museum\": \"Historiska museet\", \"F\\u00f6rem\\u0...  \n",
       "2  {\"Museum\": \"Historiska museet\", \"F\\u00f6rem\\u0...  \n",
       "3  {\"Museum\": \"Historiska museet\", \"F\\u00f6rem\\u0...  \n",
       "4  {\"Museum\": \"Historiska museet\", \"F\\u00f6rem\\u0...  "
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Replace '-' with NaNs in the entire DataFrame\n",
    "df.replace('-', np.NaN, inplace=True)\n",
    "\n",
    "df.head()  # Display the first few rows of the table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are my own functions that help clean the data\n",
    "\n",
    "def search_columns_for_substring(columns, substring):\n",
    "    return [col for col in columns if substring in col]\n",
    "\n",
    "# this function counts how many empty values are in each column for a data set (use this after the drop col function to verify)\n",
    "def count_nans_in_dataframe(df):\n",
    "    nan_counts = df.isna().sum()\n",
    "    return pd.DataFrame({'Column': nan_counts.index, 'NaN Count': nan_counts.values})\n",
    "\n",
    "# this function drops columns that do not meet the minimum threshold\n",
    "# if by=\"count\" then drop columns that don't have at least that many populated fields\n",
    "    # ie. by_val=100 drops columns that have less than 100 populated rows\n",
    "# if by=\"prop\" then drop columns that don't have at least by_val% populated rows\n",
    "    # ie. by_val=0.05 drops columns that aren't at least 5% populated\n",
    "# if by=\"field\" then drop columns that have more missing values than the columns specified\n",
    "    # ie. by_val=\"Last_Device_Array.anv\" will keep Last_Device_Array.anv but drop any cols that have more missing values than Last_Device_Array.anv\n",
    "def drop_columns_with_fewer_nans(df, by=\"prop\", by_val=\"0.05\"):\n",
    "    if by == \"count\":\n",
    "        threshold = float(by_val)\n",
    "    elif by == \"prop\": threshold = round(df.shape[0]*float(by_val))\n",
    "    elif by == \"field\": threshold = df.shape[0]-df[by_val].isna().sum()\n",
    "    cols_to_drop = []\n",
    "    for col in df.columns:\n",
    "        if df[col].isna().sum() > df.shape[0]-threshold:\n",
    "            cols_to_drop.append(col)\n",
    "    df = df.drop(cols_to_drop, axis=1)\n",
    "    return df\n",
    "\n",
    "# this function to drops columns with duplicate names\n",
    "def drop_duplicate_columns(df):\n",
    "    duplicates = df.columns[df.columns.duplicated(keep='first')]\n",
    "    df = df.loc[:, ~df.columns.duplicated(keep='first')]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Extra Details'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/anaconda3/envs/stat386/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/stat386/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/stat386/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Extra Details'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/aly.milne/Library/CloudStorage/OneDrive-BrighamYoungUniversity/Fall 2023/STAT 386/ST386_Final_Project/Scraped_Data/Viking_war_artifacts.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# unpack json\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtra Details\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mExtra Details\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mmap(json\u001b[38;5;241m.\u001b[39mloads)\n\u001b[1;32m      7\u001b[0m war_artifacts_exploded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mjson_normalize(dataset\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# drop rows that are not at least 25% populated\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/stat386/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/stat386/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Extra Details'"
     ]
    }
   ],
   "source": [
    "###### UNPACK JSON\n",
    "# Load the dataset\n",
    "# dataset = pd.read_csv('/Users/aly.milne/Library/CloudStorage/OneDrive-BrighamYoungUniversity/Fall 2023/STAT 386/ST386_Final_Project/Scraped_Data/Viking_war_artifacts.csv')\n",
    "\n",
    "# unpack json\n",
    "df['Extra Details'] = df['Extra Details'].map(json.loads)\n",
    "war_artifacts_exploded = pd.json_normalize(df.to_dict(orient='records'))\n",
    "\n",
    "# drop rows that are not at least 25% populated\n",
    "cleaned_war_artifacts = drop_columns_with_fewer_nans(war_artifacts_exploded, \"prop\", 0.25)\n",
    "\n",
    "# Removing 'Extra Details.' prefix from column names\n",
    "cleaned_war_artifacts.columns = cleaned_war_artifacts.columns.str.replace('Extra Details.', '', regex=False)\n",
    "\n",
    "# Applying the function to your dataframe\n",
    "cleaned_war_artifacts = drop_duplicate_columns(cleaned_war_artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the translator and cache\n",
    "translator = Translator()\n",
    "translations_cache = defaultdict(str)\n",
    "\n",
    "# Function to batch translate a list of texts\n",
    "def batch_translate(texts, src='sv', dest='en'):\n",
    "    # Filter out None values and ensure text is string\n",
    "    filtered_texts = [str(text) for text in texts if pd.notna(text)]\n",
    "    \n",
    "    # Batch processing and caching\n",
    "    batch_size = 10  # Adjust batch size as needed\n",
    "    for i in range(0, len(filtered_texts), batch_size):\n",
    "        batch = filtered_texts[i:i+batch_size]\n",
    "        untranslated_batch = [text for text in batch if text not in translations_cache]\n",
    "\n",
    "        if untranslated_batch:\n",
    "            try:\n",
    "                translations = translator.translate(untranslated_batch, src=src, dest=dest)\n",
    "                for text, translation in zip(untranslated_batch, translations):\n",
    "                    translations_cache[text] = translation.text\n",
    "            except Exception as e:  # General exception catch\n",
    "                print(f\"Error during translation: {e}\")\n",
    "                for text in untranslated_batch:\n",
    "                    translations_cache[text] = text\n",
    "\n",
    "        time.sleep(0.5)  # Respect rate limits\n",
    "\n",
    "    return [translations_cache[text] for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating column: Föremålsbenämning\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Translating column: Föremålsnr.\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m text_columns:\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranslating column: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m         df[column \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Translated\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_translate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m df_trade_translate\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 29\u001b[0m, in \u001b[0;36mbatch_translate\u001b[0;34m(texts, src, dest)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m untranslated_batch:\n\u001b[1;32m     27\u001b[0m                 translations_cache[text] \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m---> 29\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Respect rate limits\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [translations_cache[text] \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "###### TRANSLATE\n",
    "# Load your dataset\n",
    "df_war_translate = cleaned_war_artifacts\n",
    "\n",
    "# Identify text columns in the dataframes\n",
    "\n",
    "text_columns_war = df_war_translate.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Translate text columns\n",
    "for df, text_columns in [(df_war_translate, text_columns_war)]:\n",
    "    for column in text_columns:\n",
    "        print(f\"Translating column: {column}\")\n",
    "        df[column + '_Translated'] = batch_translate(df[column].tolist())\n",
    "\n",
    "df_war_translate.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode(location):\n",
    "    \"\"\" Geocode a location using Nominatim API. \"\"\"\n",
    "    url = 'https://nominatim.openstreetmap.org/search'\n",
    "    headers = {\n",
    "        'User-Agent': 'alyanngirl@gmail.com'\n",
    "    }\n",
    "    params = {\n",
    "        'q': location,\n",
    "        'format': 'json'\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        if results:\n",
    "            return results[0]['lat'], results[0]['lon']\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### GET LOCATIONS: WAR ARTIFACTS\n",
    "# Add columns for latitude and longitude\n",
    "cleaned_war_artifacts['latitude'] = None\n",
    "cleaned_war_artifacts['longitude'] = None\n",
    "\n",
    "# Iterate over the DataFrame and geocode each location\n",
    "for index, row in cleaned_war_artifacts.iterrows():\n",
    "    location = row['Plats']\n",
    "    lat, lon = geocode(location)\n",
    "    cleaned_war_artifacts.at[index, 'latitude'] = lat\n",
    "    cleaned_war_artifacts.at[index, 'longitude'] = lon\n",
    "\n",
    "    # Respect Nominatim's usage policy\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "cleaned_war_artifacts.to_csv('/Scraped_Data/war_w_locations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternates: \n",
    "# https://samlingar.shm.se/sok?type=object&productionPeriod=Vikingatid&hasImage=1&category=Arkeologisk%20samling&category=Vapen%20och%20rustningar&listType=archaeological&rows=500&offset=0\n",
    "# https://samlingar.shm.se/sok?type=object&productionPeriod=Vikingatid&hasImage=1&category=Arkeologisk%20samling&category=Handel%20och%20v%C3%A4rdem%C3%A4tare&listType=archaeological&rows=300&offset=0\n",
    "# https://samlingar.shm.se/sok?type=object&query=Vikingatid&listType=archaeological&rows=1000&offset=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "Error during translation: the JSON object must be str, bytes or bytearray, not NoneType\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Use .loc to assign the values back to avoid SettingWithCopyWarning\u001b[39;00m\n\u001b[1;32m     14\u001b[0m cleaned_war_artifacts\u001b[38;5;241m.\u001b[39mloc[start:end\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, column \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Translated\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m translated_texts\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "###### TRANSLATE: WAR ARTIFACTS\n",
    "text_columns = cleaned_war_artifacts.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Translate each text column\n",
    "for column in text_columns:\n",
    "    # Process the DataFrame in chunks\n",
    "    chunk_size = 100  # Adjust based on your data and rate limits\n",
    "    for start in range(0, cleaned_war_artifacts.shape[0], chunk_size):\n",
    "        end = start + chunk_size\n",
    "        df_slice = cleaned_war_artifacts[start:end]\n",
    "        translated_texts = batch_translate(df_slice[column].astype(str).tolist())\n",
    "        \n",
    "        # Use .loc to assign the values back to avoid SettingWithCopyWarning\n",
    "        cleaned_war_artifacts.loc[start:end-1, column + '_Translated'] = translated_texts\n",
    "        time.sleep(1)  # Respect rate limits\n",
    "\n",
    "\n",
    "\n",
    "# Save the translated DataFrame to a new file\n",
    "# translated_df.to_csv('/path/to/your/translated_dataset.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat386",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
