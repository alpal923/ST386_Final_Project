{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list to store the data for each individual page\n",
    "all_data = {}\n",
    "\n",
    "# Initialize the web driver\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "# Initial search page URL\n",
    "url = \"https://samlingar.shm.se/sok?type=object&query=Vikingatid&rows=1000&offset=0\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load (you may need to adjust the sleep duration)\n",
    "time.sleep(2)\n",
    "\n",
    "# Find the cookie disclaimer button by its aria-label\n",
    "cookie_disclaimer = driver.find_element(By.CSS_SELECTOR, '[aria-label=\"Godkänn alla kakor\"]')\n",
    "# Check if the cookie disclaimer button is displayed and then click it\n",
    "if cookie_disclaimer.is_displayed():\n",
    "    ActionChains(driver).move_to_element(cookie_disclaimer).click().perform()\n",
    "\n",
    "# Find all the search result items\n",
    "results = driver.find_elements(By.XPATH, \"//a[contains(@class,'rKAserSVOTC2Sy4lh50nJg') and contains(@class,'miFVghdNq-ew7wuzMJWdDw')]\")\n",
    "from collections import defaultdict\n",
    "\n",
    "# Dictionary to hold the title and its corresponding URL\n",
    "titles_and_urls = defaultdict(list)\n",
    "\n",
    "for result in results:\n",
    "    # Extract the aria-label attribute and split it to get the title\n",
    "    aria_label = result.get_attribute('aria-label')\n",
    "    title_split = aria_label.split(' - Föremålsbenämning:')\n",
    "    title = title_split[0].strip()\n",
    "\n",
    "    # Extract the URL\n",
    "    url = result.get_attribute('href')\n",
    "\n",
    "    # Modify title if it's a duplicate\n",
    "    if titles_and_urls[title]:\n",
    "        count = len(titles_and_urls[title]) + 1\n",
    "        title = f\"{title} ({count})\"\n",
    "    \n",
    "    # Add the title and URL to the dictionary\n",
    "    titles_and_urls[title].append(url)\n",
    "\n",
    "# Flatten the dictionary to ensure each title has a unique URL\n",
    "titles_and_urls = {k: v[0] for k, v in titles_and_urls.items()}\n",
    "\n",
    "# Dictionary to hold the data for all items\n",
    "all_items_dict = {}\n",
    "\n",
    "# Iterate over titles and URLs to fetch table data\n",
    "for title, url in titles_and_urls.items():\n",
    "    # Open the link of the result\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # wait for the page to load\n",
    "\n",
    "    # Grab the first table on the page as the data source\n",
    "    try:\n",
    "        table = pd.read_html(driver.page_source)[0]\n",
    "        row_dict = {row[0]: row[1] for row in table.itertuples(index=False)}\n",
    "        all_items_dict[title] = row_dict\n",
    "    except IndexError:\n",
    "        print(f\"No table found on page for {title}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing table for {title}: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "# You now have a dictionary with item titles as keys and table data (as dictionaries) as values\n",
    "all_data_json = json.dumps(all_items_dict, indent=4)\n",
    "\n",
    "# Convert the JSON string to a Python dictionary\n",
    "data_dict = json.loads(all_data_json)\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "\n",
    "# Reset the index to make the item titles a column\n",
    "df = df.reset_index()\n",
    "\n",
    "# Rename the former index column to 'unique_name'\n",
    "df = df.rename(columns={'index': 'unique_name'})\n",
    "df.to_csv('Viking_artifacts.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stat386",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
